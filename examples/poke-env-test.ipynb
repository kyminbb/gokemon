{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test poke-env api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install poke-env api by\n",
    "```{commandline}\n",
    "$ pip install poke-env\n",
    "```\n",
    "The poke-env requirements does not include dataclasses module, so if error occurs importing poke_env, you need to install it manually.\n",
    "\n",
    "```{commandline}\n",
    "$ pip install dataclasses\n",
    "```\n",
    "\n",
    "Once that is complete, clone the pokemon-showdown implementation\n",
    "```{commandline}\n",
    "$ git clone https://github.com/hsahovic/Pokemon-Showdown.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player\n",
    "\n",
    "Create random players.\n",
    "Before running the below code, you should be running your pokemon-showdown server on your localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.utils import cross_evaluate\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 RandomPlayer agents and battle with each other 20 times each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "players = [RandomPlayer(max_concurrent_battles=10, battle_format=\"gen4randombattle\") for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_evaluation = await cross_evaluate(players, n_challenges=20)\n",
    "\n",
    "table = [[\"-\"] + [p.username for p in players]]\n",
    "\n",
    "for p_1, results in cross_evaluation.items():\n",
    "    table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "\n",
    "print(tabulate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-damage player (Heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.player import Player\n",
    "from poke_env.environment.battle import Battle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a max damage player that chooses a move with maximum damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxDamagePlayer(Player):\n",
    "    def choose_move(self, battle: Battle) -> str:\n",
    "        # If the player can attack, it will\n",
    "        if battle.available_moves:\n",
    "            # Finds the best move among available ones\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "        # If no attack is available, a random switch will be made\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross evaluate with RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_player = RandomPlayer(battle_format=\"gen4randombattle\")\n",
    "max_damage_player = MaxDamagePlayer(battle_format=\"gen4randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await max_damage_player.battle_against(random_player, n_battles=100)\n",
    "\n",
    "print(f\"Max damage player won {max_damage_player.n_won_battles} out of 100 battles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Space: base power / damage multiplier / pokemon left / opponent pokemon left\n",
    "\n",
    "A 1d tensor of length 10\n",
    "\n",
    "Action Space: Given by agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.env_player import Gen8EnvSinglePlayer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleRLPlayer(Gen8EnvSinglePlayer):\n",
    "    def embed_battle(self, battle):\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Normalize 0~1\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        remaining_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "        remaining_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [remaining_mon_team, remaining_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            battle, fainted_value=2, hp_value=1, victory_value=30\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_player = SimpleRLPlayer(battle_format=\"gen8randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_player.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a deep q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "n_action = len(env_player.action_space)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation=\"elu\", input_shape=(1, 10,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"elu\"))\n",
    "model.add(Dense(n_action, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr=\"eps\",\n",
    "    value_max=1.0,\n",
    "    value_min=0.05,\n",
    "    value_test=0,\n",
    "    nb_steps=10000,\n",
    ")\n",
    "\n",
    "# Defining our DQN\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=len(env_player.action_space),\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    gamma=0.5,\n",
    "    target_model_update=1,\n",
    "    delta_clip=0.01,\n",
    "    enable_double_dqn=True,\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=0.00025), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAINING_STEPS = 100\n",
    "NB_EVALUATION_EPISODES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from poke_env.utils import to_id_str\n",
    "\n",
    "\n",
    "def dqn_training(player, dqn, nb_steps):\n",
    "    dqn.fit(player, nb_steps=nb_steps)\n",
    "    print(\"Fit complete\")\n",
    "    player.complete_current_battle()\n",
    "    print(\"Complete the rest\")\n",
    "    \n",
    "def dqn_evaluation(player, dqn, nb_episodes):\n",
    "    player.reset_battles()\n",
    "    dqn.test(player, nb_episodes=nb_episodes, visualize=False, verbose=False)\n",
    "\n",
    "    print(\n",
    "        \"DQN Evaluation: %d victories out of %d episodes\"\n",
    "        % (player.n_won_battles, nb_episodes)\n",
    "    )\n",
    "\n",
    "async def play_with(player, opponent):\n",
    "    player._start_new_battle = True\n",
    "    \n",
    "    async def launch_battles():\n",
    "        battles_coroutine = asyncio.gather(\n",
    "            player.send_challenges(\n",
    "                opponent=to_id_str(opponent.username),\n",
    "                n_challenges=1,\n",
    "                to_wait=opponent.logged_in,\n",
    "            ),\n",
    "            opponent.accept_challenges(\n",
    "                opponent=to_id_str(player.username), n_challenges=1\n",
    "            ),\n",
    "        )\n",
    "        await battles_coroutine\n",
    "    \n",
    "    def play():\n",
    "        dqn_training(player=player, dqn=dqn, nb_steps=NB_TRAINING_STEPS)\n",
    "        player._start_new_battle = False\n",
    "        while True:\n",
    "            try:\n",
    "                player.complete_current_battle()\n",
    "                player.reset()\n",
    "            except OSError:\n",
    "                break\n",
    "    \n",
    "    thread = Thread(target=play, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    while player._start_new_battle:\n",
    "        await launch_battles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
    "\n",
    "await play_with(env_player, opponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaulating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_evaluation(player, dqn, nb_episodes):\n",
    "    player.reset_battles()\n",
    "    dqn.test(player, nb_episodes=nb_episodes, visualize=False, verbose=False)\n",
    "\n",
    "    print(\n",
    "        \"DQN Evaluation: %d victories out of %d episodes\"\n",
    "        % (player.n_won_battles, nb_episodes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalulate():\n",
    "    dqn_evaluation(player=env_player, dqn=dqn, nb_steps=NB_TRAINING_STEPS)\n",
    "    env_player._start_new_battle = False\n",
    "    while True:\n",
    "        try:\n",
    "            env_player.complete_current_battle()\n",
    "            env_player.reset()\n",
    "        except OSError:\n",
    "            break\n",
    "            \n",
    "thread = Thread(target=play, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "while env_player._start_new_battle:\n",
    "    await launch_battles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Showdown Official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.server_configuration import ShowdownServerConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID: GokemonRox\n",
    "\n",
    "PW: gokemon\n",
    "\n",
    "이라는 showdown 계정을 만들었다. 위 계정을 이용하여 접속 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player = MaxDamagePlayer(\n",
    "    player_configuration=PlayerConfiguration(\"GokemonRox\", \"gokemon\"),\n",
    "    server_configuration=ShowdownServerConfiguration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이 agent 와 한국어로 배틀을 하고 싶다면, 사설 서버인 포다운을 이용해서 배틀을 한다.\n",
    "\n",
    "[포다운 다이렉트 서버](https://play.podown.pro/?p)\n",
    "\n",
    "위 주소로 접속해서 본인 아이디로 로그인을 하고\n",
    "GokemonRox 계정에 도전을 한다. (별도의 세팅이 없을 시 8세대 랜덤배틀)\n",
    "\n",
    "이후 아래 코드로 도전을 수락한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await player.accept_challenges(None, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
