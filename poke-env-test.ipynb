{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test poke-env api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install poke-env api by\n",
    "```{commandline}\n",
    "$ pip install poke-env\n",
    "```\n",
    "The poke-env requirements does not include dataclasses module, so if error occurs importing poke_env, you need to install it manually.\n",
    "\n",
    "```{commandline}\n",
    "$ pip install dataclasses\n",
    "```\n",
    "\n",
    "Once that is complete, clone the pokemon-showdown implementation\n",
    "```{commandline}\n",
    "$ git clone https://github.com/hsahovic/Pokemon-Showdown.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player\n",
    "\n",
    "Create random players.\n",
    "Before running the below code, you should be running your pokemon-showdown server on your localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.utils import cross_evaluate\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 RandomPlayer agents and battle with each other 20 times each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "players = [RandomPlayer(max_concurrent_battles=10, battle_format=\"gen4randombattle\") for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_evaluation = await cross_evaluate(players, n_challenges=20)\n",
    "\n",
    "table = [[\"-\"] + [p.username for p in players]]\n",
    "\n",
    "for p_1, results in cross_evaluation.items():\n",
    "    table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "\n",
    "print(tabulate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-damage player (Heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.player import Player\n",
    "from poke_env.environment.battle import Battle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a max damage player that chooses a move with maximum damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxDamagePlayer(Player):\n",
    "    def choose_move(self, battle: Battle) -> str:\n",
    "        # If the player can attack, it will\n",
    "        if battle.available_moves:\n",
    "            # Finds the best move among available ones\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "        # If no attack is available, a random switch will be made\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross evaluate with RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_player = RandomPlayer(battle_format=\"gen4randombattle\")\n",
    "max_damage_player = MaxDamagePlayer(battle_format=\"gen4randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await max_damage_player.battle_against(random_player, n_battles=100)\n",
    "\n",
    "print(f\"Max damage player won {max_damage_player.n_won_battles} out of 100 battles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Space: base power / damage multiplier / pokemon left / opponent pokemon left\n",
    "\n",
    "A 1d tensor of length 10\n",
    "\n",
    "Action Space: Given by agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.player.env_player import Gen8EnvSinglePlayer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleRLPlayer(Gen8EnvSinglePlayer):\n",
    "    def embed_battle(self, battle):\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Normalize 0~1\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        remaining_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "        remaining_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        return np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [remaining_mon_team, remaining_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            battle, fainted_value=2, hp_value=1, victory_value=30\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_player = SimpleRLPlayer(battle_format=\"gen8randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_player.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a deep q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "n_action = len(env_player.action_space)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation=\"elu\", input_shape=(1, 10,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"elu\"))\n",
    "model.add(Dense(n_action, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr=\"eps\",\n",
    "    value_max=1.0,\n",
    "    value_min=0.05,\n",
    "    value_test=0,\n",
    "    nb_steps=10000,\n",
    ")\n",
    "\n",
    "# Defining our DQN\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=len(env_player.action_space),\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    nb_steps_warmup=1000,\n",
    "    gamma=0.5,\n",
    "    target_model_update=1,\n",
    "    delta_clip=0.01,\n",
    "    enable_double_dqn=True,\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(lr=0.00025), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRAINING_STEPS = 10000\n",
    "NB_EVALUATION_EPISODES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shit it does not work in Ipython\n",
    "def dqn_training(player, dqn, nb_steps):\n",
    "    dqn.fit(player, nb_steps=nb_steps)\n",
    "    # This call will finished eventual unfinshed battles before returning\n",
    "    player.complete_current_battle()\n",
    "\n",
    "opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
    "\n",
    "# Training\n",
    "env_player.play_against(\n",
    "    env_algorithm=dqn_training,\n",
    "    opponent=opponent,\n",
    "    env_algorithm_kwargs={\"dqn\": dqn, \"nb_steps\": NB_TRAINING_STEPS},\n",
    ")\n",
    "model.save(f\"model_{NB_TRAINING_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Showdown Official\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.server_configuration import ShowdownServerConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID: GokemonRox\n",
    "\n",
    "PW: gokemon\n",
    "\n",
    "이라는 showdown 계정을 만들었다. 위 계정을 이용하여 접속 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 13:37:15,354 - GokemonRox - WARNING - Unhandled message: |queryresponse|debug|true\n",
      "\n",
      "2020-12-23 13:37:15,357 - GokemonRox - WARNING - Unhandled message: |queryresponse|debug|\"connect2\"\n"
     ]
    }
   ],
   "source": [
    "player = MaxDamagePlayer(\n",
    "    player_configuration=PlayerConfiguration(\"GokemonRox\", \"gokemon\"),\n",
    "    server_configuration=ShowdownServerConfiguration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이 agent 와 한국어로 배틀을 하고 싶다면, 사설 서버인 포다운을 이용해서 배틀을 한다.\n",
    "\n",
    "[포다운 다이렉트 서버](https://play.podown.pro/?p)\n",
    "\n",
    "위 주소로 접속해서 본인 아이디로 로그인을 하고\n",
    "GokemonRox 계정에 도전을 한다. (별도의 세팅이 없을 시 8세대 랜덤배틀)\n",
    "\n",
    "이후 아래 코드로 도전을 수락한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await player.accept_challenges(None, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:developer] *",
   "language": "python",
   "name": "conda-env-developer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
